{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_linear_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQUbzwtrJAmO",
        "colab_type": "text"
      },
      "source": [
        "# Linear Model - Intution\n",
        "\n",
        "This [video](https://www.youtube.com/watch?v=JvS2triCgOY) is quite helpful in understanding fundamental of regression. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIRR0KFeFw0l",
        "colab_type": "text"
      },
      "source": [
        "#### EXAMPLE 1: \n",
        "Here all calculation are done manually, without using any frameworks utilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_-OVojzMGFE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6dbe902c-5454-4d9f-e1b7-77228486c51a"
      },
      "source": [
        "#\n",
        "# Input data\n",
        "# x = [1, 2, 3, 4, 5]\n",
        "# y = [2, 4, 5, 4, 5]\n",
        "# Expected output\n",
        "# w = 0.6\n",
        "\n",
        "import torch \n",
        " \n",
        "X = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 5, 4, 5], dtype=torch.float32)\n",
        "w = torch.tensor([0.0], dtype=torch.float32)\n",
        "b = torch.tensor([0.0], dtype=torch.float32)\n",
        "\n",
        "\n",
        "# model output\n",
        "def forward(x):\n",
        "    return w * x + b\n",
        "\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_pred):\n",
        "    return ((y_pred - y)**2).mean()\n",
        "\n",
        "\n",
        "# J = MSE = 1/N * (w*x + b - y)**2\n",
        "# dJ/dw = 1/N * 2x(w*x + b - y)\n",
        "# dJ/db = i/N * 2(w*x + b - y)\n",
        "def gradient(X, Y, y_pred):\n",
        "    m = torch.dot(2*X, y_pred - Y).mean()\n",
        "    c = (2*(y_pred - Y)).mean()\n",
        "    return m, c \n",
        "\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 1500\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # predict = forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # manual gradient\n",
        "    dm, dc = gradient(X, Y, y_pred)\n",
        "\n",
        "    # nudge weights\n",
        "    w -= learning_rate * dm\n",
        "    b -= learning_rate * dc\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print('epoch {}: w = {:.3f}, loss = {:.8f}'.format((epoch+1), w.item(), l.item()))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1: w = 1.320, loss = 17.20000076\n",
            "epoch 11: w = 1.162, loss = 1.25153673\n",
            "epoch 21: w = 1.142, loss = 1.19809222\n",
            "epoch 31: w = 1.123, loss = 1.14834964\n",
            "epoch 41: w = 1.104, loss = 1.10205269\n",
            "epoch 51: w = 1.087, loss = 1.05896294\n",
            "epoch 61: w = 1.069, loss = 1.01885784\n",
            "epoch 71: w = 1.053, loss = 0.98153085\n",
            "epoch 81: w = 1.037, loss = 0.94678956\n",
            "epoch 91: w = 1.021, loss = 0.91445494\n",
            "epoch 101: w = 1.007, loss = 0.88436019\n",
            "epoch 111: w = 0.992, loss = 0.85635012\n",
            "epoch 121: w = 0.978, loss = 0.83028013\n",
            "epoch 131: w = 0.965, loss = 0.80601609\n",
            "epoch 141: w = 0.952, loss = 0.78343278\n",
            "epoch 151: w = 0.940, loss = 0.76241392\n",
            "epoch 161: w = 0.928, loss = 0.74285096\n",
            "epoch 171: w = 0.916, loss = 0.72464311\n",
            "epoch 181: w = 0.905, loss = 0.70769674\n",
            "epoch 191: w = 0.894, loss = 0.69192415\n",
            "epoch 201: w = 0.884, loss = 0.67724413\n",
            "epoch 211: w = 0.874, loss = 0.66358083\n",
            "epoch 221: w = 0.864, loss = 0.65086401\n",
            "epoch 231: w = 0.855, loss = 0.63902825\n",
            "epoch 241: w = 0.846, loss = 0.62801236\n",
            "epoch 251: w = 0.837, loss = 0.61775935\n",
            "epoch 261: w = 0.829, loss = 0.60821676\n",
            "epoch 271: w = 0.821, loss = 0.59933513\n",
            "epoch 281: w = 0.813, loss = 0.59106874\n",
            "epoch 291: w = 0.806, loss = 0.58337486\n",
            "epoch 301: w = 0.798, loss = 0.57621413\n",
            "epoch 311: w = 0.791, loss = 0.56954944\n",
            "epoch 321: w = 0.785, loss = 0.56334615\n",
            "epoch 331: w = 0.778, loss = 0.55757272\n",
            "epoch 341: w = 0.772, loss = 0.55219913\n",
            "epoch 351: w = 0.766, loss = 0.54719800\n",
            "epoch 361: w = 0.760, loss = 0.54254311\n",
            "epoch 371: w = 0.754, loss = 0.53821099\n",
            "epoch 381: w = 0.749, loss = 0.53417861\n",
            "epoch 391: w = 0.744, loss = 0.53042567\n",
            "epoch 401: w = 0.739, loss = 0.52693260\n",
            "epoch 411: w = 0.734, loss = 0.52368152\n",
            "epoch 421: w = 0.729, loss = 0.52065575\n",
            "epoch 431: w = 0.724, loss = 0.51783961\n",
            "epoch 441: w = 0.720, loss = 0.51521838\n",
            "epoch 451: w = 0.716, loss = 0.51277888\n",
            "epoch 461: w = 0.712, loss = 0.51050830\n",
            "epoch 471: w = 0.708, loss = 0.50839484\n",
            "epoch 481: w = 0.704, loss = 0.50642800\n",
            "epoch 491: w = 0.700, loss = 0.50459731\n",
            "epoch 501: w = 0.697, loss = 0.50289357\n",
            "epoch 511: w = 0.693, loss = 0.50130749\n",
            "epoch 521: w = 0.690, loss = 0.49983153\n",
            "epoch 531: w = 0.687, loss = 0.49845776\n",
            "epoch 541: w = 0.684, loss = 0.49717927\n",
            "epoch 551: w = 0.681, loss = 0.49598938\n",
            "epoch 561: w = 0.678, loss = 0.49488169\n",
            "epoch 571: w = 0.675, loss = 0.49385080\n",
            "epoch 581: w = 0.673, loss = 0.49289140\n",
            "epoch 591: w = 0.670, loss = 0.49199829\n",
            "epoch 601: w = 0.668, loss = 0.49116731\n",
            "epoch 611: w = 0.665, loss = 0.49039370\n",
            "epoch 621: w = 0.663, loss = 0.48967376\n",
            "epoch 631: w = 0.661, loss = 0.48900357\n",
            "epoch 641: w = 0.659, loss = 0.48838004\n",
            "epoch 651: w = 0.656, loss = 0.48779941\n",
            "epoch 661: w = 0.654, loss = 0.48725915\n",
            "epoch 671: w = 0.653, loss = 0.48675632\n",
            "epoch 681: w = 0.651, loss = 0.48628831\n",
            "epoch 691: w = 0.649, loss = 0.48585278\n",
            "epoch 701: w = 0.647, loss = 0.48544732\n",
            "epoch 711: w = 0.646, loss = 0.48506990\n",
            "epoch 721: w = 0.644, loss = 0.48471874\n",
            "epoch 731: w = 0.642, loss = 0.48439199\n",
            "epoch 741: w = 0.641, loss = 0.48408777\n",
            "epoch 751: w = 0.639, loss = 0.48380452\n",
            "epoch 761: w = 0.638, loss = 0.48354101\n",
            "epoch 771: w = 0.637, loss = 0.48329562\n",
            "epoch 781: w = 0.635, loss = 0.48306736\n",
            "epoch 791: w = 0.634, loss = 0.48285502\n",
            "epoch 801: w = 0.633, loss = 0.48265713\n",
            "epoch 811: w = 0.632, loss = 0.48247305\n",
            "epoch 821: w = 0.631, loss = 0.48230171\n",
            "epoch 831: w = 0.630, loss = 0.48214227\n",
            "epoch 841: w = 0.629, loss = 0.48199385\n",
            "epoch 851: w = 0.628, loss = 0.48185587\n",
            "epoch 861: w = 0.627, loss = 0.48172730\n",
            "epoch 871: w = 0.626, loss = 0.48160759\n",
            "epoch 881: w = 0.625, loss = 0.48149616\n",
            "epoch 891: w = 0.624, loss = 0.48139256\n",
            "epoch 901: w = 0.623, loss = 0.48129615\n",
            "epoch 911: w = 0.622, loss = 0.48120648\n",
            "epoch 921: w = 0.621, loss = 0.48112273\n",
            "epoch 931: w = 0.621, loss = 0.48104507\n",
            "epoch 941: w = 0.620, loss = 0.48097271\n",
            "epoch 951: w = 0.619, loss = 0.48090521\n",
            "epoch 961: w = 0.619, loss = 0.48084244\n",
            "epoch 971: w = 0.618, loss = 0.48078418\n",
            "epoch 981: w = 0.617, loss = 0.48072982\n",
            "epoch 991: w = 0.617, loss = 0.48067933\n",
            "epoch 1001: w = 0.616, loss = 0.48063231\n",
            "epoch 1011: w = 0.616, loss = 0.48058844\n",
            "epoch 1021: w = 0.615, loss = 0.48054776\n",
            "epoch 1031: w = 0.614, loss = 0.48050976\n",
            "epoch 1041: w = 0.614, loss = 0.48047453\n",
            "epoch 1051: w = 0.613, loss = 0.48044148\n",
            "epoch 1061: w = 0.613, loss = 0.48041087\n",
            "epoch 1071: w = 0.613, loss = 0.48038253\n",
            "epoch 1081: w = 0.612, loss = 0.48035598\n",
            "epoch 1091: w = 0.612, loss = 0.48033142\n",
            "epoch 1101: w = 0.611, loss = 0.48030853\n",
            "epoch 1111: w = 0.611, loss = 0.48028708\n",
            "epoch 1121: w = 0.610, loss = 0.48026714\n",
            "epoch 1131: w = 0.610, loss = 0.48024860\n",
            "epoch 1141: w = 0.610, loss = 0.48023137\n",
            "epoch 1151: w = 0.609, loss = 0.48021531\n",
            "epoch 1161: w = 0.609, loss = 0.48020059\n",
            "epoch 1171: w = 0.609, loss = 0.48018655\n",
            "epoch 1181: w = 0.608, loss = 0.48017374\n",
            "epoch 1191: w = 0.608, loss = 0.48016149\n",
            "epoch 1201: w = 0.608, loss = 0.48015037\n",
            "epoch 1211: w = 0.608, loss = 0.48014006\n",
            "epoch 1221: w = 0.607, loss = 0.48013037\n",
            "epoch 1231: w = 0.607, loss = 0.48012131\n",
            "epoch 1241: w = 0.607, loss = 0.48011294\n",
            "epoch 1251: w = 0.607, loss = 0.48010516\n",
            "epoch 1261: w = 0.606, loss = 0.48009795\n",
            "epoch 1271: w = 0.606, loss = 0.48009104\n",
            "epoch 1281: w = 0.606, loss = 0.48008466\n",
            "epoch 1291: w = 0.606, loss = 0.48007870\n",
            "epoch 1301: w = 0.605, loss = 0.48007345\n",
            "epoch 1311: w = 0.605, loss = 0.48006821\n",
            "epoch 1321: w = 0.605, loss = 0.48006362\n",
            "epoch 1331: w = 0.605, loss = 0.48005924\n",
            "epoch 1341: w = 0.605, loss = 0.48005500\n",
            "epoch 1351: w = 0.605, loss = 0.48005128\n",
            "epoch 1361: w = 0.604, loss = 0.48004779\n",
            "epoch 1371: w = 0.604, loss = 0.48004442\n",
            "epoch 1381: w = 0.604, loss = 0.48004121\n",
            "epoch 1391: w = 0.604, loss = 0.48003840\n",
            "epoch 1401: w = 0.604, loss = 0.48003587\n",
            "epoch 1411: w = 0.604, loss = 0.48003334\n",
            "epoch 1421: w = 0.604, loss = 0.48003095\n",
            "epoch 1431: w = 0.603, loss = 0.48002896\n",
            "epoch 1441: w = 0.603, loss = 0.48002690\n",
            "epoch 1451: w = 0.603, loss = 0.48002490\n",
            "epoch 1461: w = 0.603, loss = 0.48002329\n",
            "epoch 1471: w = 0.603, loss = 0.48002177\n",
            "epoch 1481: w = 0.603, loss = 0.48002011\n",
            "epoch 1491: w = 0.603, loss = 0.48001876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyIGU1LpGLIh",
        "colab_type": "text"
      },
      "source": [
        "#### EXAMPLE 2: \n",
        "Here we utilize `autograd` instead of manual gradient calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7R0gADZVE5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "85d14f57-7b3d-465c-b3cb-1f605fc5e9e6"
      },
      "source": [
        "#\n",
        "# Input data\n",
        "# x = [1, 2, 3, 4, 5]\n",
        "# y = [2, 4, 5, 4, 5]\n",
        "# Expected output\n",
        "# w = 0.6\n",
        "\n",
        "import torch\n",
        " \n",
        "X = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 5, 4, 5], dtype=torch.float32)\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "b = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model output\n",
        "def forward(x):\n",
        "    return w * x + b\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_pred):\n",
        "    return ((y_pred - y)**2).mean()\n",
        "\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 1500\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # predict = forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # calculate gradients = backward pass\n",
        "    l.backward()\n",
        "\n",
        "    # nudge the weights\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "    \n",
        "    # zero the gradients after updating\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print('epoch {}: w = {:.3f}, loss = {:.8f}'.format((epoch+1), w.item(), l.item()))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1: w = 0.264, loss = 17.20000076\n",
            "epoch 11: w = 1.046, loss = 1.15947938\n",
            "epoch 21: w = 1.083, loss = 1.04747438\n",
            "epoch 31: w = 1.071, loss = 1.01000512\n",
            "epoch 41: w = 1.055, loss = 0.97529316\n",
            "epoch 51: w = 1.040, loss = 0.94285601\n",
            "epoch 61: w = 1.026, loss = 0.91254342\n",
            "epoch 71: w = 1.011, loss = 0.88421595\n",
            "epoch 81: w = 0.998, loss = 0.85774314\n",
            "epoch 91: w = 0.984, loss = 0.83300459\n",
            "epoch 101: w = 0.972, loss = 0.80988616\n",
            "epoch 111: w = 0.959, loss = 0.78828156\n",
            "epoch 121: w = 0.947, loss = 0.76809204\n",
            "epoch 131: w = 0.936, loss = 0.74922460\n",
            "epoch 141: w = 0.925, loss = 0.73159295\n",
            "epoch 151: w = 0.914, loss = 0.71511579\n",
            "epoch 161: w = 0.903, loss = 0.69971794\n",
            "epoch 171: w = 0.893, loss = 0.68532854\n",
            "epoch 181: w = 0.883, loss = 0.67188132\n",
            "epoch 191: w = 0.874, loss = 0.65931481\n",
            "epoch 201: w = 0.865, loss = 0.64757133\n",
            "epoch 211: w = 0.856, loss = 0.63659698\n",
            "epoch 221: w = 0.848, loss = 0.62634122\n",
            "epoch 231: w = 0.839, loss = 0.61675727\n",
            "epoch 241: w = 0.831, loss = 0.60780084\n",
            "epoch 251: w = 0.824, loss = 0.59943104\n",
            "epoch 261: w = 0.816, loss = 0.59160930\n",
            "epoch 271: w = 0.809, loss = 0.58429998\n",
            "epoch 281: w = 0.802, loss = 0.57746941\n",
            "epoch 291: w = 0.795, loss = 0.57108587\n",
            "epoch 301: w = 0.789, loss = 0.56512070\n",
            "epoch 311: w = 0.782, loss = 0.55954605\n",
            "epoch 321: w = 0.776, loss = 0.55433643\n",
            "epoch 331: w = 0.771, loss = 0.54946828\n",
            "epoch 341: w = 0.765, loss = 0.54491866\n",
            "epoch 351: w = 0.759, loss = 0.54066706\n",
            "epoch 361: w = 0.754, loss = 0.53669405\n",
            "epoch 371: w = 0.749, loss = 0.53298104\n",
            "epoch 381: w = 0.744, loss = 0.52951127\n",
            "epoch 391: w = 0.739, loss = 0.52626860\n",
            "epoch 401: w = 0.735, loss = 0.52323866\n",
            "epoch 411: w = 0.730, loss = 0.52040690\n",
            "epoch 421: w = 0.726, loss = 0.51776040\n",
            "epoch 431: w = 0.722, loss = 0.51528758\n",
            "epoch 441: w = 0.717, loss = 0.51297653\n",
            "epoch 451: w = 0.714, loss = 0.51081693\n",
            "epoch 461: w = 0.710, loss = 0.50879854\n",
            "epoch 471: w = 0.706, loss = 0.50691253\n",
            "epoch 481: w = 0.703, loss = 0.50515008\n",
            "epoch 491: w = 0.699, loss = 0.50350291\n",
            "epoch 501: w = 0.696, loss = 0.50196391\n",
            "epoch 511: w = 0.693, loss = 0.50052530\n",
            "epoch 521: w = 0.690, loss = 0.49918112\n",
            "epoch 531: w = 0.687, loss = 0.49792495\n",
            "epoch 541: w = 0.684, loss = 0.49675101\n",
            "epoch 551: w = 0.681, loss = 0.49565405\n",
            "epoch 561: w = 0.678, loss = 0.49462885\n",
            "epoch 571: w = 0.676, loss = 0.49367085\n",
            "epoch 581: w = 0.673, loss = 0.49277535\n",
            "epoch 591: w = 0.671, loss = 0.49193874\n",
            "epoch 601: w = 0.668, loss = 0.49115700\n",
            "epoch 611: w = 0.666, loss = 0.49042621\n",
            "epoch 621: w = 0.664, loss = 0.48974338\n",
            "epoch 631: w = 0.662, loss = 0.48910531\n",
            "epoch 641: w = 0.660, loss = 0.48850900\n",
            "epoch 651: w = 0.658, loss = 0.48795167\n",
            "epoch 661: w = 0.656, loss = 0.48743087\n",
            "epoch 671: w = 0.654, loss = 0.48694426\n",
            "epoch 681: w = 0.652, loss = 0.48648939\n",
            "epoch 691: w = 0.650, loss = 0.48606443\n",
            "epoch 701: w = 0.649, loss = 0.48566717\n",
            "epoch 711: w = 0.647, loss = 0.48529607\n",
            "epoch 721: w = 0.646, loss = 0.48494941\n",
            "epoch 731: w = 0.644, loss = 0.48462510\n",
            "epoch 741: w = 0.643, loss = 0.48432213\n",
            "epoch 751: w = 0.641, loss = 0.48403925\n",
            "epoch 761: w = 0.640, loss = 0.48377475\n",
            "epoch 771: w = 0.638, loss = 0.48352751\n",
            "epoch 781: w = 0.637, loss = 0.48329648\n",
            "epoch 791: w = 0.636, loss = 0.48308048\n",
            "epoch 801: w = 0.635, loss = 0.48287877\n",
            "epoch 811: w = 0.634, loss = 0.48269024\n",
            "epoch 821: w = 0.632, loss = 0.48251414\n",
            "epoch 831: w = 0.631, loss = 0.48234934\n",
            "epoch 841: w = 0.630, loss = 0.48219556\n",
            "epoch 851: w = 0.629, loss = 0.48205179\n",
            "epoch 861: w = 0.628, loss = 0.48191747\n",
            "epoch 871: w = 0.627, loss = 0.48179191\n",
            "epoch 881: w = 0.626, loss = 0.48167443\n",
            "epoch 891: w = 0.626, loss = 0.48156482\n",
            "epoch 901: w = 0.625, loss = 0.48146230\n",
            "epoch 911: w = 0.624, loss = 0.48136654\n",
            "epoch 921: w = 0.623, loss = 0.48127714\n",
            "epoch 931: w = 0.622, loss = 0.48119339\n",
            "epoch 941: w = 0.622, loss = 0.48111528\n",
            "epoch 951: w = 0.621, loss = 0.48104209\n",
            "epoch 961: w = 0.620, loss = 0.48097405\n",
            "epoch 971: w = 0.620, loss = 0.48091015\n",
            "epoch 981: w = 0.619, loss = 0.48085064\n",
            "epoch 991: w = 0.618, loss = 0.48079500\n",
            "epoch 1001: w = 0.618, loss = 0.48074293\n",
            "epoch 1011: w = 0.617, loss = 0.48069412\n",
            "epoch 1021: w = 0.616, loss = 0.48064870\n",
            "epoch 1031: w = 0.616, loss = 0.48060617\n",
            "epoch 1041: w = 0.615, loss = 0.48056644\n",
            "epoch 1051: w = 0.615, loss = 0.48052940\n",
            "epoch 1061: w = 0.614, loss = 0.48049459\n",
            "epoch 1071: w = 0.614, loss = 0.48046240\n",
            "epoch 1081: w = 0.613, loss = 0.48043212\n",
            "epoch 1091: w = 0.613, loss = 0.48040366\n",
            "epoch 1101: w = 0.613, loss = 0.48037735\n",
            "epoch 1111: w = 0.612, loss = 0.48035270\n",
            "epoch 1121: w = 0.612, loss = 0.48032945\n",
            "epoch 1131: w = 0.611, loss = 0.48030812\n",
            "epoch 1141: w = 0.611, loss = 0.48028773\n",
            "epoch 1151: w = 0.611, loss = 0.48026904\n",
            "epoch 1161: w = 0.610, loss = 0.48025125\n",
            "epoch 1171: w = 0.610, loss = 0.48023486\n",
            "epoch 1181: w = 0.610, loss = 0.48021954\n",
            "epoch 1191: w = 0.609, loss = 0.48020521\n",
            "epoch 1201: w = 0.609, loss = 0.48019162\n",
            "epoch 1211: w = 0.609, loss = 0.48017925\n",
            "epoch 1221: w = 0.608, loss = 0.48016739\n",
            "epoch 1231: w = 0.608, loss = 0.48015651\n",
            "epoch 1241: w = 0.608, loss = 0.48014611\n",
            "epoch 1251: w = 0.608, loss = 0.48013654\n",
            "epoch 1261: w = 0.607, loss = 0.48012757\n",
            "epoch 1271: w = 0.607, loss = 0.48011923\n",
            "epoch 1281: w = 0.607, loss = 0.48011154\n",
            "epoch 1291: w = 0.607, loss = 0.48010430\n",
            "epoch 1301: w = 0.606, loss = 0.48009744\n",
            "epoch 1311: w = 0.606, loss = 0.48009092\n",
            "epoch 1321: w = 0.606, loss = 0.48008496\n",
            "epoch 1331: w = 0.606, loss = 0.48007947\n",
            "epoch 1341: w = 0.606, loss = 0.48007426\n",
            "epoch 1351: w = 0.605, loss = 0.48006946\n",
            "epoch 1361: w = 0.605, loss = 0.48006487\n",
            "epoch 1371: w = 0.605, loss = 0.48006058\n",
            "epoch 1381: w = 0.605, loss = 0.48005667\n",
            "epoch 1391: w = 0.605, loss = 0.48005289\n",
            "epoch 1401: w = 0.605, loss = 0.48004937\n",
            "epoch 1411: w = 0.604, loss = 0.48004627\n",
            "epoch 1421: w = 0.604, loss = 0.48004311\n",
            "epoch 1431: w = 0.604, loss = 0.48004031\n",
            "epoch 1441: w = 0.604, loss = 0.48003769\n",
            "epoch 1451: w = 0.604, loss = 0.48003536\n",
            "epoch 1461: w = 0.604, loss = 0.48003292\n",
            "epoch 1471: w = 0.604, loss = 0.48003086\n",
            "epoch 1481: w = 0.603, loss = 0.48002869\n",
            "epoch 1491: w = 0.603, loss = 0.48002681\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_-BXGj8MjV6",
        "colab_type": "text"
      },
      "source": [
        "#### Example 3:\n",
        "Source for this dataset is [here](https://www.freecodecamp.org/news/machine-learning-mean-squared-error-regression-line-c7dde9a26b93/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBhXm73iZnm2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f0be7fa4-45ed-44ca-dae2-1fff6f1bae43"
      },
      "source": [
        "# \n",
        "# Input data\n",
        "# x = [1, 2, 4]\n",
        "# y = [2, 1, 3]\n",
        "# Expected output\n",
        "# w = 0.42\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "X = torch.tensor([1, 2, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 1, 3], dtype=torch.float32)\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "b = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model output\n",
        "def forward(x):\n",
        "    return w * x + b\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_pred):\n",
        "    return ((y_pred - y)**2).mean()\n",
        "\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 1500\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # predict = forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # calculate gradients = backward pass\n",
        "    l.backward()\n",
        "\n",
        "    # nudge the weights\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "    \n",
        "    # zero the gradients after updating\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print('epoch {}: w = {:.3f}, loss = {:.8f}'.format((epoch+1), w.item(), l.item()))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1: w = 0.107, loss = 4.66666651\n",
            "epoch 11: w = 0.572, loss = 0.64054585\n",
            "epoch 21: w = 0.650, loss = 0.49627757\n",
            "epoch 31: w = 0.656, loss = 0.48323765\n",
            "epoch 41: w = 0.650, loss = 0.47523996\n",
            "epoch 51: w = 0.642, loss = 0.46799254\n",
            "epoch 61: w = 0.634, loss = 0.46130645\n",
            "epoch 71: w = 0.626, loss = 0.45513406\n",
            "epoch 81: w = 0.618, loss = 0.44943574\n",
            "epoch 91: w = 0.611, loss = 0.44417524\n",
            "epoch 101: w = 0.603, loss = 0.43931875\n",
            "epoch 111: w = 0.597, loss = 0.43483534\n",
            "epoch 121: w = 0.590, loss = 0.43069637\n",
            "epoch 131: w = 0.584, loss = 0.42687523\n",
            "epoch 141: w = 0.578, loss = 0.42334768\n",
            "epoch 151: w = 0.572, loss = 0.42009115\n",
            "epoch 161: w = 0.566, loss = 0.41708457\n",
            "epoch 171: w = 0.561, loss = 0.41430911\n",
            "epoch 181: w = 0.556, loss = 0.41174695\n",
            "epoch 191: w = 0.551, loss = 0.40938139\n",
            "epoch 201: w = 0.546, loss = 0.40719768\n",
            "epoch 211: w = 0.541, loss = 0.40518153\n",
            "epoch 221: w = 0.537, loss = 0.40332043\n",
            "epoch 231: w = 0.533, loss = 0.40160224\n",
            "epoch 241: w = 0.529, loss = 0.40001598\n",
            "epoch 251: w = 0.525, loss = 0.39855161\n",
            "epoch 261: w = 0.521, loss = 0.39719975\n",
            "epoch 271: w = 0.517, loss = 0.39595175\n",
            "epoch 281: w = 0.514, loss = 0.39479956\n",
            "epoch 291: w = 0.510, loss = 0.39373586\n",
            "epoch 301: w = 0.507, loss = 0.39275393\n",
            "epoch 311: w = 0.504, loss = 0.39184737\n",
            "epoch 321: w = 0.501, loss = 0.39101049\n",
            "epoch 331: w = 0.498, loss = 0.39023781\n",
            "epoch 341: w = 0.496, loss = 0.38952458\n",
            "epoch 351: w = 0.493, loss = 0.38886607\n",
            "epoch 361: w = 0.490, loss = 0.38825819\n",
            "epoch 371: w = 0.488, loss = 0.38769698\n",
            "epoch 381: w = 0.486, loss = 0.38717893\n",
            "epoch 391: w = 0.483, loss = 0.38670063\n",
            "epoch 401: w = 0.481, loss = 0.38625905\n",
            "epoch 411: w = 0.479, loss = 0.38585150\n",
            "epoch 421: w = 0.477, loss = 0.38547519\n",
            "epoch 431: w = 0.475, loss = 0.38512769\n",
            "epoch 441: w = 0.474, loss = 0.38480696\n",
            "epoch 451: w = 0.472, loss = 0.38451090\n",
            "epoch 461: w = 0.470, loss = 0.38423756\n",
            "epoch 471: w = 0.468, loss = 0.38398516\n",
            "epoch 481: w = 0.467, loss = 0.38375232\n",
            "epoch 491: w = 0.465, loss = 0.38353708\n",
            "epoch 501: w = 0.464, loss = 0.38333866\n",
            "epoch 511: w = 0.463, loss = 0.38315532\n",
            "epoch 521: w = 0.461, loss = 0.38298610\n",
            "epoch 531: w = 0.460, loss = 0.38282990\n",
            "epoch 541: w = 0.459, loss = 0.38268563\n",
            "epoch 551: w = 0.458, loss = 0.38255250\n",
            "epoch 561: w = 0.456, loss = 0.38242963\n",
            "epoch 571: w = 0.455, loss = 0.38231611\n",
            "epoch 581: w = 0.454, loss = 0.38221145\n",
            "epoch 591: w = 0.453, loss = 0.38211474\n",
            "epoch 601: w = 0.452, loss = 0.38202539\n",
            "epoch 611: w = 0.451, loss = 0.38194299\n",
            "epoch 621: w = 0.450, loss = 0.38186684\n",
            "epoch 631: w = 0.450, loss = 0.38179660\n",
            "epoch 641: w = 0.449, loss = 0.38173175\n",
            "epoch 651: w = 0.448, loss = 0.38167188\n",
            "epoch 661: w = 0.447, loss = 0.38161659\n",
            "epoch 671: w = 0.446, loss = 0.38156560\n",
            "epoch 681: w = 0.446, loss = 0.38151857\n",
            "epoch 691: w = 0.445, loss = 0.38147500\n",
            "epoch 701: w = 0.444, loss = 0.38143483\n",
            "epoch 711: w = 0.444, loss = 0.38139781\n",
            "epoch 721: w = 0.443, loss = 0.38136360\n",
            "epoch 731: w = 0.443, loss = 0.38133204\n",
            "epoch 741: w = 0.442, loss = 0.38130280\n",
            "epoch 751: w = 0.442, loss = 0.38127598\n",
            "epoch 761: w = 0.441, loss = 0.38125110\n",
            "epoch 771: w = 0.441, loss = 0.38122812\n",
            "epoch 781: w = 0.440, loss = 0.38120699\n",
            "epoch 791: w = 0.440, loss = 0.38118735\n",
            "epoch 801: w = 0.439, loss = 0.38116932\n",
            "epoch 811: w = 0.439, loss = 0.38115272\n",
            "epoch 821: w = 0.438, loss = 0.38113728\n",
            "epoch 831: w = 0.438, loss = 0.38112310\n",
            "epoch 841: w = 0.438, loss = 0.38110998\n",
            "epoch 851: w = 0.437, loss = 0.38109788\n",
            "epoch 861: w = 0.437, loss = 0.38108668\n",
            "epoch 871: w = 0.437, loss = 0.38107634\n",
            "epoch 881: w = 0.436, loss = 0.38106680\n",
            "epoch 891: w = 0.436, loss = 0.38105807\n",
            "epoch 901: w = 0.436, loss = 0.38104990\n",
            "epoch 911: w = 0.435, loss = 0.38104239\n",
            "epoch 921: w = 0.435, loss = 0.38103548\n",
            "epoch 931: w = 0.435, loss = 0.38102913\n",
            "epoch 941: w = 0.435, loss = 0.38102329\n",
            "epoch 951: w = 0.434, loss = 0.38101777\n",
            "epoch 961: w = 0.434, loss = 0.38101283\n",
            "epoch 971: w = 0.434, loss = 0.38100806\n",
            "epoch 981: w = 0.434, loss = 0.38100383\n",
            "epoch 991: w = 0.434, loss = 0.38099983\n",
            "epoch 1001: w = 0.433, loss = 0.38099623\n",
            "epoch 1011: w = 0.433, loss = 0.38099289\n",
            "epoch 1021: w = 0.433, loss = 0.38098979\n",
            "epoch 1031: w = 0.433, loss = 0.38098690\n",
            "epoch 1041: w = 0.433, loss = 0.38098428\n",
            "epoch 1051: w = 0.432, loss = 0.38098180\n",
            "epoch 1061: w = 0.432, loss = 0.38097951\n",
            "epoch 1071: w = 0.432, loss = 0.38097748\n",
            "epoch 1081: w = 0.432, loss = 0.38097548\n",
            "epoch 1091: w = 0.432, loss = 0.38097373\n",
            "epoch 1101: w = 0.432, loss = 0.38097215\n",
            "epoch 1111: w = 0.432, loss = 0.38097063\n",
            "epoch 1121: w = 0.432, loss = 0.38096917\n",
            "epoch 1131: w = 0.431, loss = 0.38096794\n",
            "epoch 1141: w = 0.431, loss = 0.38096669\n",
            "epoch 1151: w = 0.431, loss = 0.38096559\n",
            "epoch 1161: w = 0.431, loss = 0.38096461\n",
            "epoch 1171: w = 0.431, loss = 0.38096371\n",
            "epoch 1181: w = 0.431, loss = 0.38096276\n",
            "epoch 1191: w = 0.431, loss = 0.38096201\n",
            "epoch 1201: w = 0.431, loss = 0.38096118\n",
            "epoch 1211: w = 0.431, loss = 0.38096055\n",
            "epoch 1221: w = 0.431, loss = 0.38095996\n",
            "epoch 1231: w = 0.430, loss = 0.38095930\n",
            "epoch 1241: w = 0.430, loss = 0.38095880\n",
            "epoch 1251: w = 0.430, loss = 0.38095832\n",
            "epoch 1261: w = 0.430, loss = 0.38095787\n",
            "epoch 1271: w = 0.430, loss = 0.38095748\n",
            "epoch 1281: w = 0.430, loss = 0.38095710\n",
            "epoch 1291: w = 0.430, loss = 0.38095674\n",
            "epoch 1301: w = 0.430, loss = 0.38095632\n",
            "epoch 1311: w = 0.430, loss = 0.38095602\n",
            "epoch 1321: w = 0.430, loss = 0.38095579\n",
            "epoch 1331: w = 0.430, loss = 0.38095555\n",
            "epoch 1341: w = 0.430, loss = 0.38095525\n",
            "epoch 1351: w = 0.430, loss = 0.38095501\n",
            "epoch 1361: w = 0.430, loss = 0.38095483\n",
            "epoch 1371: w = 0.430, loss = 0.38095465\n",
            "epoch 1381: w = 0.430, loss = 0.38095441\n",
            "epoch 1391: w = 0.430, loss = 0.38095430\n",
            "epoch 1401: w = 0.430, loss = 0.38095418\n",
            "epoch 1411: w = 0.430, loss = 0.38095400\n",
            "epoch 1421: w = 0.429, loss = 0.38095388\n",
            "epoch 1431: w = 0.429, loss = 0.38095388\n",
            "epoch 1441: w = 0.429, loss = 0.38095370\n",
            "epoch 1451: w = 0.429, loss = 0.38095355\n",
            "epoch 1461: w = 0.429, loss = 0.38095346\n",
            "epoch 1471: w = 0.429, loss = 0.38095334\n",
            "epoch 1481: w = 0.429, loss = 0.38095334\n",
            "epoch 1491: w = 0.429, loss = 0.38095322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVmA8NIjNYKl",
        "colab_type": "text"
      },
      "source": [
        "#### Example 4:\n",
        "Source for this dataset is [here](https://www.freecodecamp.org/news/machine-learning-mean-squared-error-regression-line-c7dde9a26b93/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykshUgMjaKAo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "91ced64f-74dd-42f9-8494-60ff08ea6e01"
      },
      "source": [
        "# \n",
        "# Input data\n",
        "# x = [-2, -1, 1, 4]\n",
        "# y = [-3, -1, 2, 3]\n",
        "# Expected output\n",
        "# w = 0.97\n",
        "\n",
        "import torch\n",
        "\n",
        "X = torch.tensor([-2, -1, 1, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([-3, -1, 2, 3], dtype=torch.float32)\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "b = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model output\n",
        "def forward(x):\n",
        "    return w * x + b\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_pred):\n",
        "    return ((y_pred - y)**2).mean()\n",
        "\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 300\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # predict = forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # calculate gradients = backward pass\n",
        "    l.backward()\n",
        "\n",
        "    # nudge the weights\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "    \n",
        "    # zero the gradients after updating\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print('epoch {}: w = {:.3f}, loss = {:.8f}'.format((epoch+1), w.item(), l.item()))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1: w = 0.105, loss = 5.75000000\n",
            "epoch 11: w = 0.689, loss = 1.23047090\n",
            "epoch 21: w = 0.872, loss = 0.78098881\n",
            "epoch 31: w = 0.931, loss = 0.72417641\n",
            "epoch 41: w = 0.952, loss = 0.70910966\n",
            "epoch 51: w = 0.960, loss = 0.70107329\n",
            "epoch 61: w = 0.964, loss = 0.69580019\n",
            "epoch 71: w = 0.966, loss = 0.69222033\n",
            "epoch 81: w = 0.968, loss = 0.68977809\n",
            "epoch 91: w = 0.970, loss = 0.68811113\n",
            "epoch 101: w = 0.971, loss = 0.68697298\n",
            "epoch 111: w = 0.972, loss = 0.68619585\n",
            "epoch 121: w = 0.972, loss = 0.68566543\n",
            "epoch 131: w = 0.973, loss = 0.68530321\n",
            "epoch 141: w = 0.974, loss = 0.68505591\n",
            "epoch 151: w = 0.974, loss = 0.68488711\n",
            "epoch 161: w = 0.974, loss = 0.68477178\n",
            "epoch 171: w = 0.975, loss = 0.68469304\n",
            "epoch 181: w = 0.975, loss = 0.68463945\n",
            "epoch 191: w = 0.975, loss = 0.68460274\n",
            "epoch 201: w = 0.975, loss = 0.68457770\n",
            "epoch 211: w = 0.976, loss = 0.68456054\n",
            "epoch 221: w = 0.976, loss = 0.68454897\n",
            "epoch 231: w = 0.976, loss = 0.68454099\n",
            "epoch 241: w = 0.976, loss = 0.68453550\n",
            "epoch 251: w = 0.976, loss = 0.68453187\n",
            "epoch 261: w = 0.976, loss = 0.68452919\n",
            "epoch 271: w = 0.976, loss = 0.68452752\n",
            "epoch 281: w = 0.976, loss = 0.68452632\n",
            "epoch 291: w = 0.976, loss = 0.68452561\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}